The primary motive to use DSA is to solve a problem effectively and efficiently.

efficeiency is mesaured in terms of complexities:

- time complexity: time reqd to execute a program
- space complexity: space taken by program's variables
  - auxilary space complexity: space taken by variables apart from the input params
- big O used to measure the complexity
  - compuatation growth w.r.t. the i/p
  - O(n) -> linear growth, loop's length grows n grows, constants are dropped, 2 sequential loops don't make O(2n), but O(n) [n^2 matters though]
  - practical caveat: in sort insertion vs quick sort; smaller n^2 faster than a much larger dropped constant
  - even in early exits loops, worst case scenario is n, therefore O(n)
  - common complexities: O(1) [1 can be any other number], O(logn), O(nlogn) [quicksort], O(n^2), O(2^n), O(sqrt(n))
- imp concepts:

  - growth is w.r.t. the i/p
  - constants are dropped
  - always consider the worst case scenario

- arrays

  - contiguos memory space
  - getting something at an index: width of the type is multiplied by the offset, a[0], here offset is zero, assume width is 8 bits, O(1), size width and offset are constant/fixed, don't grow w/ the i/p
  - insertion: go to a's mem address, width offset, and overwrite
  - deletion: go to a's mem address, width offset, and set to nullish
  - technically insertt, push, pop aren't available in static or traditional arrays(which needs fixed need to be passed in), so [] isn't technically an array!
  - has .length prooperty

  - linear search:

    - indexOf does linear search, O(n) since complexity grows w/ input n

  - binary search:

    - check if dataset is ordered?
    - time complexity O(log n), since we halve the array and traverse till 1 element is left
    - if we are scanning the input, the timecomplexity becomes O(nlogn)

  - two crystal balls dropped from the building, optimised way to find where they'll break?, at which floor if top is given? use O(sqrt(n)) coomplexity
