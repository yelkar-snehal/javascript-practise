The primary motive to use DSA is to solve a problem effectively and efficiently.

efficeiency is mesaured in terms of complexities:

- time complexity: time reqd to execute a program
- space complexity: space taken by program's variables
  - auxilary space complexity: space taken by variables apart from the input params
- big O used to measure the complexity
  - compuatation growth w.r.t. the i/p
  - O(n) -> linear growth, loop's length grows n grows, constants are dropped, 2 sequential loops don't make O(2n), but O(n) [n^2 matters though]
  - practical caveat: in sort insertion vs quick sort; smaller n^2 faster than a much larger dropped constant
  - even in early exits loops, worst case scenario is n, therefore O(n)
  - common complexities: O(1) [1 can be any other number], O(logn), O(nlogn) [quicksort], O(n^2), O(2^n), O(sqrt(n))
- imp concepts:

  - growth is w.r.t. the i/p
  - constants are dropped
  - always consider the worst case scenario

- arrays

  - contiguos memory space
  - getting something at an index: width of the type is multiplied by the offset, a[0], here offset is zero, assume width is 8 bits, O(1), size width and offset are constant/fixed, don't grow w/ the i/p
  - insertion: go to a's mem address, width offset, and overwrite
  - deletion: go to a's mem address, width offset, and set to nullish
  - technically insertt, push, pop aren't available in static or traditional arrays(which needs fixed need to be passed in), so [] isn't technically an array!
  - has .length prooperty

  - seraching:

    - linear search:

      - indexOf does linear search, O(n) since complexity grows w/ input n

    - binary search:

      - check if dataset is ordered?
      - time complexity O(log n), since we halve the array and traverse till 1 element is left
      - if we are scanning the input, the timecomplexity becomes O(nlogn)

    - two crystal balls dropped from the building, optimised way to find where they'll break?, at which floor if top is given? use O(sqrt(n)) coomplexity

  - sorting: X[i] <= X[i + 1]

    - bubble sort:
      - time complexity, n(n + 1)/2 => dropping insignificant additions makes it O(n^2)

- linked lists

  - O(1) deletion, inssertion
  - however if insertion deletion depends on the traversal of the linked list, O(n)
  - non contiguos memory allocation
  - singly/doubly depending on prev of a node
  - node can be of a generic type T which has next and/or prev to oint to another node

- miscellaneous

  - Note: You must wrap the code in a function, otherwise the error will not be caught and the assertion will fail. jestjs.io/docs expect#tothrowerror
  - buffer is similar to arraylist, also slice method on both will return a shallow copy of the data structure, i.e., they refer to the same contiguos memory location with different indices

- queue

  - ds on top of a linked list
  - first in first out
  - constraining the insertion and removal logic, no need to traverse
  - therefore O(1), constant not considering JS time to access obj properties (might be logn)Àù
  - generally has only three methods: enqueue, deque, and peek

- stack

  - built on top of a linked list
  - first in last out
  - O(1), constant not considering JS time to access obj properties (might be logn)
  - need to traverse backwards, hence new links are formed first and then head is moved
  - used mostly in resursion, stack trace of fn in erros

- array vs linked list

  - usability
  - time
  - space
  - example: frontend apis has to be limited to 5, so array would require frequent shit and unshift; this can be instead be done w/ a queue(linked list)

- ArrayList

  - which is what [] is in JS
  - since get, push and pop are constant with time complexity O(1), but shift and unshift grow linearly with time complexity O(n)
  - index based operations are faster with the time complexity of O(1)
  - operations like stack, pop and push are also faster with the time complexity of O(1), since the length is fixed and independent of the input
  - however operations like enqueue and deque can be slower if there are many elements in the arraylist, since we've to shift and unshift the elements

- Recursion

  - base case is the most important, ie the exit case
  - return address, return value and arguments to the function have to be considered
  - 3 steps: pre, recurse, and post
  - generally go down the stack first, then after hitting the base case, traverse up the stack

- Quicksort

  - uses divide and conquer approach
  - very useful when procecssing power is less
  - divide and conquer strategy

- Tress
  - start from a point and grown to many, i.e, many connections unlike a linear list or stack or queue
  - like multiple lists
  - eventually all data structures lead to trees
  - use recursion
  - e.g: file system, DOM, compiler
  - babel tree example ref(https://astexplorer.net/)
  - root is the topmost node/point
  - height is the length from the root to the grandest descendant, in general height of strands can vary
  - binary tree, at max two children for each node
  - binary search tree, strong ordering
  - leaf, grandest descendant
  - balanced tree: all strands w/ same height
  - branching factor, possible number of child nodes parent node has
  - traversal: pre-order, post-order, in-order
  - strong vs weak ordering: levl vs node ordering
  - time complexity: O(n) since traversing grows linearly w/ i/p
